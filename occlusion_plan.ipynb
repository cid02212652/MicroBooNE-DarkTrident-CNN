{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3cb059bd-b282-4fa8-8650-12f481a548dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Where all your inference folders live\n",
    "INFER_BASE = Path(\"/home/hep/an1522/dark_tridents_wspace/outputs/inference\")\n",
    "\n",
    "# Where you want the occlusion selection CSVs to go\n",
    "OUT_BASE = INFER_BASE / \"_occlusion_selections\"\n",
    "OUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Your expected columns in *_scores.csv\n",
    "KEYS = [\"run_number\", \"subrun_number\", \"event_number\"]\n",
    "NEEDED = KEYS + [\"signal_score\", \"entry_number\", \"n_pixels\"]\n",
    "\n",
    "# Sentinel / invalid values observed in your tables\n",
    "BAD_SCORE = -999999.9\n",
    "BAD_PIXELS = -1\n",
    "\n",
    "# n_pixels minimum cut (from your distribution reasoning)\n",
    "# NPX_MIN = {\n",
    "#     \"samples\": 0,\n",
    "#     \"signal\": 0,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b9880a0a-a323-4d96-b95e-f24e0b99fd0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found folders: 8\n",
      "Total rows loaded: 1050708\n",
      "Folders loaded: 8\n"
     ]
    }
   ],
   "source": [
    "def list_inference_folders(base: Path):\n",
    "    # only real folders, ignore helper directories like run*_pdf/run*_png\n",
    "    folders = []\n",
    "    for p in base.iterdir():\n",
    "        if not p.is_dir():\n",
    "            continue\n",
    "        name = p.name\n",
    "        if name.endswith(\"_pdf\") or name.endswith(\"_png\"):\n",
    "            continue\n",
    "        if name.startswith(\"_\"):\n",
    "            continue\n",
    "        if name.endswith(\"_resnet18_bn\"):\n",
    "            continue\n",
    "        if name.endswith(\"_resnet18_gn\"):\n",
    "            continue\n",
    "        if name.endswith(\"_resnet34_bn\"):\n",
    "            continue\n",
    "        folders.append(p)\n",
    "    return sorted(folders)\n",
    "\n",
    "def read_scores_in_folder(folder: Path):\n",
    "    # read all *_scores.csv in that folder\n",
    "    files = sorted(folder.glob(\"*_scores.csv\"))\n",
    "    if not files:\n",
    "        return None\n",
    "    \n",
    "    dfs = []\n",
    "    for f in files:\n",
    "        df = pd.read_csv(f)\n",
    "        missing = [c for c in NEEDED if c not in df.columns]\n",
    "        if missing:\n",
    "            # skip files that aren't in the expected format\n",
    "            continue\n",
    "        df = df[NEEDED].copy()\n",
    "        df[\"__file\"] = f.name\n",
    "        df[\"__folder\"] = folder.name\n",
    "        dfs.append(df)\n",
    "    if not dfs:\n",
    "        return None\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "folders = list_inference_folders(INFER_BASE)\n",
    "print(\"Found folders:\", len(folders))\n",
    "# print(\"Folder names:\", folders)\n",
    "\n",
    "all_dfs = []\n",
    "for folder in folders:\n",
    "    df = read_scores_in_folder(folder)\n",
    "    if df is not None:\n",
    "        all_dfs.append(df)\n",
    "\n",
    "all_scores = pd.concat(all_dfs, ignore_index=True)\n",
    "print(\"Total rows loaded:\", len(all_scores))\n",
    "print(\"Folders loaded:\", all_scores[\"__folder\"].nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "51e8889a-72a3-4f06-9d94-8179368053df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After signal-file filter: 133264\n",
      "__run  __kind \n",
      "run1   samples    2\n",
      "       signal     2\n",
      "run3   samples    2\n",
      "       signal     2\n",
      "Name: __folder, dtype: int64\n",
      "Samples files now present (unique): 6\n",
      "0                run1_NuMI_dirt_larcv_cropped_scores.csv\n",
      "3331       run1_NuMI_nu_overlay_larcv_cropped_scores.csv\n",
      "17101     run1_offbeam_larcv_cropped_full_set_scores.csv\n",
      "459464                run3_dirt_larcv_cropped_scores.csv\n",
      "461877          run3_nu_overlay_larcv_cropped_scores.csv\n",
      "473403             run3_offbeam_larcv_cropped_scores.csv\n",
      "Name: __file, dtype: object\n",
      "Signal files now present (unique): 2\n",
      "81802     run1_dt_ratio_0.6_ma_0.05_pi0_larcv_cropped_sc...\n",
      "576111    run3_dt_ratio_0.6_ma_0.05_pi0_larcv_cropped_sc...\n",
      "Name: __file, dtype: object\n"
     ]
    }
   ],
   "source": [
    "def dataset_type_from_folder(folder_name: str):\n",
    "    # Expecting folder names like:\n",
    "    # run1_samples, run1_samples_resnet18_bn, run1_signal_resnet34_gn, etc.\n",
    "    # We'll classify into (run, kind) where kind in {\"samples\",\"signal\"}\n",
    "    parts = folder_name.split(\"_\")\n",
    "    run = parts[0]  # \"run1\" or \"run3\"\n",
    "    # find whether it contains \"samples\" or \"signal\"\n",
    "    kind = \"samples\" if \"samples\" in parts else (\"signal\" if \"signal\" in parts else None)\n",
    "    return run, kind\n",
    "\n",
    "all_scores[[\"__run\", \"__kind\"]] = all_scores[\"__folder\"].apply(\n",
    "    lambda s: pd.Series(dataset_type_from_folder(s))\n",
    ")\n",
    "\n",
    "# Keep only run1/run3 samples/signal\n",
    "all_scores = all_scores.dropna(subset=[\"__run\", \"__kind\"]).copy()\n",
    "\n",
    "# Clean invalid sentinel rows\n",
    "mask_good = (\n",
    "    (all_scores[\"signal_score\"] != BAD_SCORE) &\n",
    "    (all_scores[\"n_pixels\"] != BAD_PIXELS) &\n",
    "    (all_scores[\"n_pixels\"] >= 0)\n",
    ")\n",
    "clean = all_scores.loc[mask_good].copy()\n",
    "\n",
    "SIGNAL_KEEP = \"dt_ratio_0.6_ma_0.05_pi0\"\n",
    "\n",
    "# Keep:\n",
    "# - ALL samples rows\n",
    "# - ONLY signal rows whose __file contains SIGNAL_KEEP\n",
    "clean = clean[\n",
    "    (clean[\"__kind\"] == \"samples\") |\n",
    "    ((clean[\"__kind\"] == \"signal\") & clean[\"__file\"].str.contains(SIGNAL_KEEP, na=False))\n",
    "].copy()\n",
    "\n",
    "print(\"After signal-file filter:\", len(clean))\n",
    "print(clean.groupby([\"__run\",\"__kind\"])[\"__folder\"].nunique())\n",
    "print(\"Samples files now present (unique):\", clean.loc[clean[\"__kind\"]==\"samples\",\"__file\"].nunique())\n",
    "print(clean.loc[clean[\"__kind\"]==\"samples\", \"__file\"].drop_duplicates().head(10))\n",
    "print(\"Signal files now present (unique):\", clean.loc[clean[\"__kind\"]==\"signal\",\"__file\"].nunique())\n",
    "print(clean.loc[clean[\"__kind\"]==\"signal\", \"__file\"].drop_duplicates().head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "205b2dd3-97d8-45e8-a0f7-975bd86a26c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scores_to_root(scores_file: str, root_base_dir: str):\n",
    "    \"\"\"\n",
    "    scores_file: e.g. 'run1_dt_ratio_0.6_ma_0.05_pi0_larcv_cropped_scores.csv'\n",
    "    returns: '/vols/.../run1_signal/run1_dt_ratio_0.6_ma_0.05_pi0_larcv_cropped.root'\n",
    "    \"\"\"\n",
    "    stem = Path(scores_file).name.replace(\"_scores.csv\", \"\")\n",
    "    return str(Path(root_base_dir) / f\"{stem}.root\")\n",
    "    \n",
    "def select_occlusion_set(df_folder, kind,\n",
    "                         A=10, B=10, C=10, D=5,\n",
    "                         # tail selection fractions\n",
    "                         samples_high_q=0.995,     # top 0.5% for samples (outliers)\n",
    "                         signal_low_q=0.005,       # bottom 0.5% for signal (failures)\n",
    "                         # borderline target score (usually 0.5 for sigmoid)\n",
    "                         borderline_target=0.5,\n",
    "                         border_frac=0.01,         # closest 1% to target\n",
    "                         # weird: within chosen tail, pick lowest 10% n_pixels\n",
    "                         weird_lowpix_q=0.10):\n",
    "    \"\"\"\n",
    "    kind: \"samples\" or \"signal\"\n",
    "    Returns a deduped set of events to occlude for ONE folder.\n",
    "    \"\"\"\n",
    "    df = df_folder.copy()\n",
    "\n",
    "    # # Require sensible pixels\n",
    "    # df = df[df[\"n_pixels\"].ge(NPX_MIN[kind])].copy()\n",
    "    # if df.empty:\n",
    "    #     return df\n",
    "\n",
    "    # A) Tail events (different for samples vs signal)\n",
    "    if kind == \"samples\":\n",
    "        q = df[\"signal_score\"].quantile(samples_high_q)\n",
    "        A_df = df[df[\"signal_score\"] >= q].sort_values(\"signal_score\", ascending=False).head(A)\n",
    "    else:  # signal\n",
    "        q = df[\"signal_score\"].quantile(signal_low_q)\n",
    "        A_df = df[df[\"signal_score\"] <= q].sort_values(\"signal_score\", ascending=True).head(A)\n",
    "\n",
    "    # B) Borderline: closest to 0.5 (or whatever you set)\n",
    "    df2 = df.assign(__dist=(df[\"signal_score\"] - borderline_target).abs())\n",
    "    k = max(1, int(border_frac * len(df2)))\n",
    "    B_df = df2.sort_values(\"__dist\", ascending=True).head(max(B, k)).drop(columns=\"__dist\").head(B)\n",
    "\n",
    "    # C) Weird: within the tail from A_df, pick those with low pixels\n",
    "    if len(A_df) > 0:\n",
    "        pix_cut = A_df[\"n_pixels\"].quantile(weird_lowpix_q)\n",
    "        C_df = A_df[A_df[\"n_pixels\"] <= pix_cut].sort_values(\n",
    "            [\"n_pixels\", \"signal_score\"],\n",
    "            ascending=[True, (kind == \"signal\")]  # signal tail is low scores\n",
    "        ).head(C)\n",
    "    else:\n",
    "        C_df = df.iloc[0:0].copy()\n",
    "\n",
    "    # D) Controls:\n",
    "    # samples: low score but high pixels (busy background confidently background)\n",
    "    # signal: high score but high pixels (easy signal)\n",
    "    pix_hi = df[\"n_pixels\"].quantile(0.75)\n",
    "    busy = df[df[\"n_pixels\"] >= pix_hi].copy()\n",
    "    if kind == \"samples\":\n",
    "        D_df = busy.sort_values(\"signal_score\", ascending=True).head(D)\n",
    "    else:\n",
    "        D_df = busy.sort_values(\"signal_score\", ascending=False).head(D)\n",
    "\n",
    "    # Combine + dedupe\n",
    "    pick = pd.concat([A_df, B_df, C_df, D_df], ignore_index=True)\n",
    "    pick = pick.drop_duplicates(subset=KEYS, keep=\"first\")\n",
    "\n",
    "    # Label reasons (simple + consistent)\n",
    "    def reason(row):\n",
    "        tags = []\n",
    "        if kind == \"samples\":\n",
    "            if row[\"signal_score\"] >= df[\"signal_score\"].quantile(samples_high_q):\n",
    "                tags.append(\"A_high_tail\")\n",
    "        else:\n",
    "            if row[\"signal_score\"] <= df[\"signal_score\"].quantile(signal_low_q):\n",
    "                tags.append(\"A_low_tail\")\n",
    "\n",
    "        # borderline tag\n",
    "        if abs(row[\"signal_score\"] - borderline_target) <= (df[\"signal_score\"] - borderline_target).abs().quantile(border_frac):\n",
    "            tags.append(\"B_border_0p5\")\n",
    "\n",
    "        if len(A_df) and (row[\"n_pixels\"] <= A_df[\"n_pixels\"].quantile(weird_lowpix_q)):\n",
    "            tags.append(\"C_weird_lowpix\")\n",
    "\n",
    "        if row[\"n_pixels\"] >= pix_hi:\n",
    "            tags.append(\"D_busy\")\n",
    "\n",
    "        return \"+\".join(sorted(set(tags))) if tags else \"picked\"\n",
    "\n",
    "    pick[\"pick_reason\"] = pick.apply(reason, axis=1)\n",
    "\n",
    "    # Make sure these metadata columns are present in the result\n",
    "    for col in [\"__file\", \"__folder\", \"__run\", \"__kind\"]:\n",
    "        if col not in pick.columns and col in df_folder.columns:\n",
    "            pick[col] = df_folder[col].iloc[0]\n",
    "\n",
    "    return pick\n",
    "\n",
    "def select_occlusion_set_stratified(df_folder, kind: str,\n",
    "                                   files=None,\n",
    "                                   per_file_counts=(4, 4, 4, 2),  # A,B,C,D per file\n",
    "                                   **kwargs):\n",
    "    \"\"\"\n",
    "    For samples: enforce representation by selecting A/B/C/D per __file group.\n",
    "    For signal: usually files will be length-1 anyway after filtering.\n",
    "    \"\"\"\n",
    "    A, B, C, D = per_file_counts\n",
    "\n",
    "    df = df_folder.copy()\n",
    "    if files is not None:\n",
    "        df = df[df[\"__file\"].isin(files)].copy()\n",
    "\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    picks = []\n",
    "    for f, g in df.groupby(\"__file\"):\n",
    "        p = select_occlusion_set(g, kind=kind, A=A, B=B, C=C, D=D, **kwargs)\n",
    "        if p is not None and not p.empty:\n",
    "            picks.append(p)\n",
    "\n",
    "    if not picks:\n",
    "        return df.iloc[0:0].copy()\n",
    "\n",
    "    out = pd.concat(picks, ignore_index=True)\n",
    "    out = out.drop_duplicates(subset=KEYS, keep=\"first\")\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2fe67c40-6de8-4e38-acf1-147ba2cb6b0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_disagreement_picks(\n",
    "    picks_ref: pd.DataFrame,\n",
    "    run: str,\n",
    "    kind: str,\n",
    "    other_folder: str,\n",
    "    E_abs: int = 10,\n",
    "    E_flip: int = 10,\n",
    "    thr: float = 0.5,\n",
    "    margin: float = 0.20,\n",
    "):\n",
    "    \"\"\"\n",
    "    Add 'model disagreement' events between reference folder (picks_ref comes from ref folder)\n",
    "    and another folder (e.g. run1_samples_resnet34_gn).\n",
    "\n",
    "    Returns: (picks_plus, extra_debug_df)\n",
    "    - picks_plus is picks_ref with extra rows appended (deduped) and pick_reason tagged.\n",
    "    - extra_debug_df contains the disagreement candidates and their deltas (for inspection).\n",
    "    \"\"\"\n",
    "    # Determine reference folder name from picks_ref metadata\n",
    "    if \"__folder\" not in picks_ref.columns:\n",
    "        raise ValueError(\"picks_ref must contain __folder column (you already propagate it).\")\n",
    "    ref_folder = picks_ref[\"__folder\"].iloc[0]\n",
    "\n",
    "    # All rows for ref + other folder\n",
    "    subset = clean[(clean[\"__run\"] == run) & (clean[\"__kind\"] == kind)].copy()\n",
    "\n",
    "    df_ref   = subset[subset[\"__folder\"] == ref_folder].copy()\n",
    "    df_other = subset[subset[\"__folder\"] == other_folder].copy()\n",
    "    if df_other.empty or df_ref.empty:\n",
    "        print(f\"[warn] disagreement: missing data for {ref_folder} or {other_folder}\")\n",
    "        return picks_ref, pd.DataFrame()\n",
    "\n",
    "    # Minimal columns\n",
    "    df_ref   = df_ref[KEYS + [\"signal_score\", \"n_pixels\", \"__file\", \"__folder\", \"entry_number\"]].copy()\n",
    "    df_other = df_other[KEYS + [\"signal_score\"]].copy()\n",
    "\n",
    "    df_ref = df_ref.rename(columns={\"signal_score\": \"score_ref\"})\n",
    "    df_other = df_other.rename(columns={\"signal_score\": \"score_other\"})\n",
    "\n",
    "    m = df_ref.merge(df_other, on=KEYS, how=\"inner\")\n",
    "    if m.empty:\n",
    "        print(\"[warn] disagreement: no overlap on KEYS\")\n",
    "        return picks_ref, pd.DataFrame()\n",
    "\n",
    "    # disagreement metrics\n",
    "    m[\"delta\"] = m[\"score_other\"] - m[\"score_ref\"]\n",
    "    m[\"abs_delta\"] = m[\"delta\"].abs()\n",
    "\n",
    "    # Strong flip: requires being confidently on opposite sides\n",
    "    m[\"flip_strong\"] = (\n",
    "        ((m[\"score_ref\"]   >= thr + margin) & (m[\"score_other\"] <= thr - margin)) |\n",
    "        ((m[\"score_other\"] >= thr + margin) & (m[\"score_ref\"]   <= thr - margin))\n",
    "    )\n",
    "\n",
    "    # Pick top |delta| + top strong flips\n",
    "    top_abs  = m.sort_values(\"abs_delta\", ascending=False).head(E_abs)\n",
    "    top_flip = m[m[\"flip_strong\"]].sort_values(\"abs_delta\", ascending=False).head(E_flip)\n",
    "\n",
    "    extra = pd.concat([top_abs, top_flip], ignore_index=True)\n",
    "    extra = extra.drop_duplicates(subset=KEYS, keep=\"first\")\n",
    "\n",
    "    # Convert back into \"event rows\" (needs same schema as your picks)\n",
    "    extra_rows = subset.merge(extra[KEYS], on=KEYS, how=\"inner\")\n",
    "\n",
    "    # Tag reasons (append to any existing pick_reason if present)\n",
    "    def mk_reason(row):\n",
    "        # find the disagreement row for this event\n",
    "        rr = extra.loc[\n",
    "            (extra[\"run_number\"] == row[\"run_number\"]) &\n",
    "            (extra[\"subrun_number\"] == row[\"subrun_number\"]) &\n",
    "            (extra[\"event_number\"] == row[\"event_number\"])\n",
    "        ].iloc[0]\n",
    "        tags = [\"E_disagree\", f\"E_vs_{other_folder}\"]\n",
    "        if rr[\"flip_strong\"]:\n",
    "            tags.append(f\"E_strong_flip_m{margin}\")\n",
    "        # which model higher\n",
    "        tags.append(\"E_other_gt_ref\" if rr[\"delta\"] > 0 else \"E_ref_gt_other\")\n",
    "        return \"+\".join(tags)\n",
    "\n",
    "    if \"pick_reason\" not in extra_rows.columns:\n",
    "        extra_rows[\"pick_reason\"] = \"\"\n",
    "\n",
    "    extra_rows[\"pick_reason\"] = extra_rows.apply(mk_reason, axis=1)\n",
    "\n",
    "    # Combine + dedupe with original picks_ref\n",
    "    out = pd.concat([picks_ref, extra_rows], ignore_index=True)\n",
    "    out = out.drop_duplicates(subset=KEYS, keep=\"first\")\n",
    "\n",
    "    return out, m  # m is nice to inspect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ada63883-1348-41e7-95c3-b72b32f5bfb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote: /home/hep/an1522/dark_tridents_wspace/outputs/inference/_occlusion_selections/master__run1_samples.csv rows= 40\n",
      "Wrote: /home/hep/an1522/dark_tridents_wspace/outputs/inference/_occlusion_selections/master__run1_signal.csv rows= 35\n",
      "Wrote: /home/hep/an1522/dark_tridents_wspace/outputs/inference/_occlusion_selections/master__run3_samples.csv rows= 39\n",
      "Wrote: /home/hep/an1522/dark_tridents_wspace/outputs/inference/_occlusion_selections/master__run3_signal.csv rows= 35\n"
     ]
    }
   ],
   "source": [
    "RUN1_SAMPLES_FILES = [\n",
    "    \"run1_NuMI_dirt_larcv_cropped_scores.csv\",\n",
    "    \"run1_NuMI_nu_overlay_larcv_cropped_scores.csv\",\n",
    "    \"run1_offbeam_larcv_cropped_full_set_scores.csv\",\n",
    "]\n",
    "\n",
    "def make_master_for(run: str, kind: str, reference_folder: str = None):\n",
    "    subset = clean[(clean[\"__run\"] == run) & (clean[\"__kind\"] == kind)].copy()\n",
    "    folders = sorted(subset[\"__folder\"].unique().tolist())\n",
    "    if not folders:\n",
    "        print(f\"[warn] no data for {run}_{kind}\")\n",
    "        return None\n",
    "\n",
    "    ref = reference_folder if (reference_folder in folders) else folders[0]\n",
    "    ref_clean = ref.replace(f\"{run}_{kind}_\", \"\")\n",
    "    df_ref = subset[subset[\"__folder\"] == ref].copy()\n",
    "\n",
    "    # ---- Choose selection strategy ----\n",
    "    if kind == \"samples\":\n",
    "        # pick a few per file so you always include dirt + nu_overlay + offbeam\n",
    "        files = RUN1_SAMPLES_FILES if run == \"run1\" else None  # define RUN3 similarly if you want\n",
    "        picks = select_occlusion_set_stratified(\n",
    "            df_ref, kind=\"samples\",\n",
    "            files=files,\n",
    "            per_file_counts=(4,4,4,2),   # totals: 3 files * (4+4+4+2)=42 max, then dedupe\n",
    "        )\n",
    "    else:\n",
    "        # signal already filtered to ONE file via SIG_KEEP\n",
    "        picks = select_occlusion_set(df_ref, kind=\"signal\")\n",
    "\n",
    "    if picks is None or picks.empty:\n",
    "        print(f\"[warn] no picks for {run}_{kind} (ref={ref})\")\n",
    "        return None\n",
    "\n",
    "    # --- Add disagreement events: MPID (ref) vs ResNet34_GN ---\n",
    "    if kind in [\"samples\", \"signal\"]:\n",
    "        other = f\"{run}_{kind}_resnet34_gn\"   # e.g. run1_samples_resnet34_gn\n",
    "        picks, disagree_debug = add_disagreement_picks(\n",
    "            picks_ref=picks,\n",
    "            run=run,\n",
    "            kind=kind,\n",
    "            other_folder=other,\n",
    "            E_abs=10,\n",
    "            E_flip=10,\n",
    "            margin=0.20,\n",
    "        )\n",
    "\n",
    "    # ---- Build master ----\n",
    "    keep = [\"pick_reason\", \"entry_number\", \"__file\"]\n",
    "    master = picks[keep].copy()\n",
    "    \n",
    "    # Merge signal scores from all folders\n",
    "    for folder in folders:\n",
    "        folder_clean = folder.replace(f\"{run}_{kind}_\", \"\")\n",
    "        df_folder = subset[subset[\"__folder\"] == folder].copy()\n",
    "        \n",
    "        # Merge on entry_number AND __file to match the right rows\n",
    "        merge_on = [\"entry_number\", \"__file\"]\n",
    "        df_folder = df_folder[merge_on + [\"signal_score\"]].rename(\n",
    "            columns={\"signal_score\": f\"{folder_clean}__signal_score\"}\n",
    "        )\n",
    "        master = master.merge(df_folder, on=merge_on, how=\"left\")\n",
    "\n",
    "    master[\"n_pixels\"] = picks[\"n_pixels\"].copy()\n",
    "    master = master.rename(columns={\"__file\": \"scores_file\"})\n",
    "    scores_file_col = master.pop(\"scores_file\")\n",
    "    master[\"scores_file\"] = scores_file_col\n",
    "    master[\"reference_folder\"] = ref_clean\n",
    "    master[\"dataset\"] = f\"{run}_{kind}\"\n",
    "    master[KEYS] = picks[KEYS].copy()\n",
    "    return master\n",
    "\n",
    "masters = {}\n",
    "for run in [\"run1\",\"run3\"]:\n",
    "    for kind in [\"samples\",\"signal\"]:\n",
    "        m = make_master_for(run, kind)\n",
    "        if m is not None:\n",
    "            masters[f\"{run}_{kind}\"] = m\n",
    "            out = OUT_BASE / f\"master__{run}_{kind}.csv\"\n",
    "            m.to_csv(out, index=False)\n",
    "            print(\"Wrote:\", out, \"rows=\", len(m))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "78dcb814-607c-4829-82c1-b9572d3d12ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote report: /home/hep/an1522/dark_tridents_wspace/outputs/inference/_occlusion_selections/matching_report.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>folder</th>\n",
       "      <th>total_master_events</th>\n",
       "      <th>matched_events</th>\n",
       "      <th>wrote_csv</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>run1_samples</td>\n",
       "      <td>mpid</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>run1_samples</td>\n",
       "      <td>resnet34_gn</td>\n",
       "      <td>40</td>\n",
       "      <td>40</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>run1_signal</td>\n",
       "      <td>mpid</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>run1_signal</td>\n",
       "      <td>resnet34_gn</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>run3_samples</td>\n",
       "      <td>mpid</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>run3_samples</td>\n",
       "      <td>resnet34_gn</td>\n",
       "      <td>39</td>\n",
       "      <td>39</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>run3_signal</td>\n",
       "      <td>mpid</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>run3_signal</td>\n",
       "      <td>resnet34_gn</td>\n",
       "      <td>35</td>\n",
       "      <td>35</td>\n",
       "      <td>/home/hep/an1522/dark_tridents_wspace/outputs/...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        dataset       folder  total_master_events  matched_events  \\\n",
       "0  run1_samples         mpid                   40              40   \n",
       "1  run1_samples  resnet34_gn                   40              40   \n",
       "2   run1_signal         mpid                   35              35   \n",
       "3   run1_signal  resnet34_gn                   35              35   \n",
       "4  run3_samples         mpid                   39              39   \n",
       "5  run3_samples  resnet34_gn                   39              39   \n",
       "6   run3_signal         mpid                   35              35   \n",
       "7   run3_signal  resnet34_gn                   35              35   \n",
       "\n",
       "                                           wrote_csv  \n",
       "0  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "1  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "2  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "3  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "4  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "5  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "6  /home/hep/an1522/dark_tridents_wspace/outputs/...  \n",
       "7  /home/hep/an1522/dark_tridents_wspace/outputs/...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ROOT_BASE = \"/vols/sbn/uboone/darkTridents/data/larcv_files\"\n",
    "\n",
    "def root_dir_for(run: str, kind: str):\n",
    "    return str(Path(ROOT_BASE) / f\"{run}_{kind}\")\n",
    "    \n",
    "def write_to_occlude_files(master: pd.DataFrame):\n",
    "    dataset = master[\"dataset\"].iloc[0]\n",
    "    run, kind = dataset.split(\"_\", 1)\n",
    "\n",
    "    subset = clean[(clean[\"__run\"] == run) & (clean[\"__kind\"] == kind)].copy()\n",
    "    folders = sorted(subset[\"__folder\"].unique().tolist())\n",
    "\n",
    "    report_rows = []\n",
    "    for folder in folders:\n",
    "        df_f = subset[subset[\"__folder\"] == folder].copy()\n",
    "\n",
    "        lookup = df_f[KEYS + [\"entry_number\", \"signal_score\", \"n_pixels\", \"__file\"]].copy()\n",
    "        lookup = lookup.rename(columns={\"__file\": \"scores_file\"})\n",
    "        lookup = lookup.drop_duplicates(subset=KEYS, keep=\"first\")\n",
    "\n",
    "        # Add suffixes to handle duplicate columns\n",
    "        merged = master.merge(lookup, on=KEYS, how=\"left\", suffixes=(\"_ref\", \"_folder\"))\n",
    "\n",
    "        # Use folder-specific values\n",
    "        matched = merged[\"entry_number_folder\"].notna().sum()\n",
    "        total = len(merged)\n",
    "\n",
    "        to_occ = merged.dropna(subset=[\"entry_number_folder\"]).copy()\n",
    "        to_occ[\"entry_number\"] = to_occ[\"entry_number_folder\"].astype(int)\n",
    "        to_occ[\"scores_file\"] = to_occ[\"scores_file_folder\"]\n",
    "\n",
    "        # Add root_file\n",
    "        root_dir = root_dir_for(run, kind)\n",
    "        to_occ[\"root_file\"] = to_occ[\"scores_file\"].apply(lambda s: scores_to_root(s, root_dir))\n",
    "\n",
    "        # Clean folder name\n",
    "        folder_clean = folder.replace(f\"{run}_{kind}_\", \"\")\n",
    "        \n",
    "        # Select final columns to save\n",
    "        keep_cols = (KEYS + [\"pick_reason\", \"entry_number\"] + \n",
    "                    [col for col in to_occ.columns if \"__signal_score\" in col] +\n",
    "                    [\"n_pixels_folder\", \"scores_file\", \"root_file\", \"reference_folder\", \"dataset\"])\n",
    "        \n",
    "        to_occ = to_occ[keep_cols].rename(columns={\"n_pixels_folder\": \"n_pixels\"})\n",
    "        \n",
    "        out = OUT_BASE / f\"to_occlude__{dataset}__{folder_clean}.csv\"\n",
    "        to_occ.to_csv(out, index=False)\n",
    "\n",
    "        report_rows.append({\n",
    "            \"dataset\": dataset,\n",
    "            \"folder\": folder_clean,\n",
    "            \"total_master_events\": total,\n",
    "            \"matched_events\": int(matched),\n",
    "            \"wrote_csv\": str(out),\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(report_rows)\n",
    "\n",
    "\n",
    "reports = []\n",
    "for name, master in masters.items():\n",
    "    rep = write_to_occlude_files(master)\n",
    "    reports.append(rep)\n",
    "\n",
    "report = pd.concat(reports, ignore_index=True)\n",
    "report_path = OUT_BASE / \"matching_report.csv\"\n",
    "report.to_csv(report_path, index=False)\n",
    "\n",
    "print(\"Wrote report:\", report_path)\n",
    "report\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f3da4b2a-6e90-481c-a4fd-9ca2cfdcbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def query_events(folder_name: str, score_min=None, score_max=None, npx_min=None, npx_max=None, top=20, sort_by=\"signal_score\"):\n",
    "    df = clean[clean[\"__folder\"] == folder_name].copy()\n",
    "    if df.empty:\n",
    "        print(\"No rows for folder:\", folder_name)\n",
    "        return df\n",
    "\n",
    "    if score_min is not None: df = df[df[\"signal_score\"] >= score_min]\n",
    "    if score_max is not None: df = df[df[\"signal_score\"] <= score_max]\n",
    "    if npx_min is not None:   df = df[df[\"n_pixels\"] >= npx_min]\n",
    "    if npx_max is not None:   df = df[df[\"n_pixels\"] <= npx_max]\n",
    "\n",
    "    if df.empty:\n",
    "        print(\"No matches.\")\n",
    "        return df\n",
    "\n",
    "    df = df.sort_values(sort_by, ascending=False if sort_by==\"signal_score\" else True)\n",
    "    return df[KEYS + [\"signal_score\",\"n_pixels\",\"entry_number\",\"__file\"]].head(top)\n",
    "\n",
    "# Example:\n",
    "# query_events(\"run1_samples_resnet34_gn\", score_min=0.5, npx_min=500, top=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DM-CNN (.venv)",
   "language": "python",
   "name": "dmc-cnn"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.25"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
